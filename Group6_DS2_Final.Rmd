---
title: "Data Science 2 Final"
author: "Group 6"
output: 
  pdf_document:
    toc: TRUE
---

\newpage

# Set up

## Load libraries and data
```{r warning=FALSE, message=FALSE}
library(caret)
library(mgcv)
library(earth)
library(tidyverse)
library(summarytools)
library(corrplot)
library(ggpubr)
library(rpart)
library(rpart.plot)
library(randomForest)
library(ranger)
library(gbm)
library(pdp)
library(vip)

setwd("D:/CUMC/Y2S2/DS2/Final/ds2_final")

load("./recovery.RData") 
```

## Subset 2 df and keep unique observations
```{r warning=FALSE, message=FALSE}
set.seed(2543) 

dat1 <- dat[sample(1:10000, 2000),]

set.seed(4017) 

dat2 <- dat[sample(1:10000, 2000),]

dat_bind <- unique(rbind(dat1, dat2))
```

# Exploratory analysis and data visualization

## Data Partition

Here, we mainly want to investigate the EDA of the training dataset. Therefore, we will start with the data partition. 

```{r warning=FALSE, message=FALSE}
set.seed(2460)

trainRows <- createDataPartition(y = dat_bind$recovery_time, 
                                 p = 0.8, list = FALSE)
```


## Understanding the outcome variable `recovery_time`
```{r warning=FALSE, message=FALSE}
# check the outcome variable
hist(dat_bind$recovery_time[trainRows], breaks = 50)
```

The distribution of the outcome variable `recovery_time` is heavily right-skewed. To account for this, we will take the log-transformation of the outcome and use that variable for following analyses.


```{r warning=FALSE, message=FALSE}
dat_bind_primary = dat_bind %>% 
  na.omit(dat_bind) %>% 
  mutate(lrecovery_time = log(recovery_time)) %>% 
  dplyr::select(-recovery_time, -id) 
  
# log-transformation helped with making it more normal
hist(dat_bind_primary$lrecovery_time[trainRows], breaks = 50)
```


## Summary of the dataset
```{r warning=FALSE, message=FALSE}
st_options(plain.ascii = F,
           style = "rmarkdown",
           dfSummary.silent = T,
           footnote = NA,
           subtitle.emphasis = F)

dfSummary(dat_bind_primary[trainRows, -1])
```


## Understand categorical variables
```{r warning=FALSE, message=FALSE}
gender = (dat_bind_primary[trainRows, -1]) %>% 
  ggplot(aes(x = gender)) +  geom_bar() +  labs(x = "Gender", y = "Count")

race = (dat_bind_primary[trainRows, -1]) %>% 
  ggplot(aes(x = race)) + geom_bar() + labs(x = "Race",y = "Count") +
  scale_x_discrete(labels = c("White", "Asian", "Black", "Hispanic"))

smoking = (dat_bind_primary[trainRows, -1]) %>% 
  ggplot(aes(x = smoking)) + geom_bar() + labs(x = "Smoking", y = "Count")

hypertension = (dat_bind_primary[trainRows, -1]) %>% 
  ggplot(aes(x = hypertension)) + geom_bar() + labs(x = "Hypertension", 
                                                    y = "Count")
diabetes = (dat_bind_primary[trainRows, -1]) %>% 
  ggplot(aes(x = diabetes)) + geom_bar() + labs(x = "Diabetes",y = "Count")

vaccine = (dat_bind_primary[trainRows, -1]) %>% 
  ggplot(aes(x = vaccine)) + geom_bar() + labs(x = "Vaccination status",
                                               y = "Count")
severity = (dat_bind_primary[trainRows, -1]) %>% 
  ggplot(aes(x = severity)) + geom_bar() + labs(x = "Severity", y = "Count")

study = (dat_bind_primary[trainRows, -1]) %>% 
  ggplot(aes(x = study)) + geom_bar() + labs(x = "Study Site", y = "Count")

cat_combined_plot = ggarrange(gender, race, smoking, hypertension, 
                               diabetes, vaccine,severity, study,
                          ncol = 2, nrow = 4)

cat_combined_plot
```


## Understand continuous variables
```{r warning=FALSE, message=FALSE}
par(mar = c(3, 3, 2, 2), mfrow = c(2, 3))

age = hist(dat_bind_primary$age[trainRows], breaks = 50)

bmi = hist(dat_bind_primary$bmi[trainRows], breaks = 50)

height = hist(dat_bind_primary$height[trainRows], breaks = 50)

weight = hist(dat_bind_primary$weight[trainRows], breaks = 50)

SBP = hist(dat_bind_primary$SBP[trainRows], breaks = 50)

LDL = hist(dat_bind_primary$LDL[trainRows], breaks = 50)
```


## Understand the correlation between continuous predictors
```{r warning=FALSE, message=FALSE}
correlation <- model.matrix(lrecovery_time ~ ., dat_bind_primary)[trainRows,-1]

corrplot(cor(dat_bind_primary[trainRows,c(1,5,6,7,10,11)]), method = "circle", type = "full")
```


## Understand the relationship with continuous predictors and the outcome
```{r warning=FALSE, message=FALSE}
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1)
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2)
trellis.par.set(theme1)

# plotting continuous predictors 
featurePlot(x = model.matrix(lrecovery_time ~ ., dat_bind_primary)[trainRows,c("age", "SBP", "LDL", "height", "weight", "bmi")],
            y = dat_bind_primary$lrecovery_time[trainRows],
            plot = "scatter",
            span = .5,
            labels = c("Predictors","Log(Y)"),
            type = c("p", "smooth"),
            layout = c(3,2))
```

## Understand the relationship between categorical predictors and continuous outcome
```{r warning=FALSE, message=FALSE}
gender3 = (dat_bind_primary[trainRows, -1]) %>% 
  ggplot(aes(x = as.factor(gender), y = lrecovery_time)) + geom_boxplot() + labs(x = "Gender", y = "Log Recovery Time") + scale_x_discrete(labels = c("Female", "Male"))

race3 = (dat_bind_primary[trainRows, -1]) %>% 
  ggplot(aes(x = as.factor(race), y = lrecovery_time)) + geom_boxplot() + labs(x = "Race", y = "Log Recovery Time") +
  scale_x_discrete(labels = c("White", "Asian", "Black", "Hispanic"))

smoking3 = (dat_bind_primary[trainRows, -1]) %>% 
  ggplot(aes(x = as.factor(smoking), y = lrecovery_time)) + geom_boxplot() + labs(x = "Smokoing Status", y = "Log Recovery Time") +
  scale_x_discrete(labels = c("Never smoked", "Former Smoker", "Current smoker"))

hypertension3 = (dat_bind_primary[trainRows, -1]) %>% 
  ggplot(aes(x = as.factor(hypertension), y = lrecovery_time)) + geom_boxplot() + labs(x = "Hypertension Status", y = "Log Recovery Time") +
  scale_x_discrete(labels = c("No", "Yes"))

diabetes3 = (dat_bind_primary[trainRows, -1]) %>% 
  ggplot(aes(x = as.factor(diabetes), y = lrecovery_time)) + geom_boxplot() + labs(x = "Diabetes Status", y = "Log Recovery Time") +
  scale_x_discrete(labels = c("No", "Yes"))

vaccine3 = (dat_bind_primary[trainRows, -1]) %>% 
  ggplot(aes(x = as.factor(vaccine), y = lrecovery_time)) + geom_boxplot() + labs(x = "Vaccination Status", y = "Log Recovery Time") +
  scale_x_discrete(labels = c("No", "Yes"))

severity3 = (dat_bind_primary[trainRows, -1]) %>% 
  ggplot(aes(x = as.factor(severity), y = lrecovery_time)) + geom_boxplot() + labs(x = "Severity", y = "Log Recovery Time") +
  scale_x_discrete(labels = c("Not severe", "Severe"))

study3 = (dat_bind_primary[trainRows, -1]) %>% 
  ggplot(aes(x = as.factor(study), y = lrecovery_time)) + geom_boxplot() + labs(x = "Study", y = "Log Recovery Time")

cat_combined_plot3 = ggarrange(gender3, race3, smoking3, hypertension3, 
                               diabetes3, vaccine3, severity3, study3,
                          ncol = 2, nrow = 4)

cat_combined_plot3
```


## Considering variables based on the EDA

From the correlation plot, we can observe that `bmi` is highly correlated with `weight` and `height`, which makes sense because BMI is calculated by weight divided by the square of height. This demonstrates collinearity between the variables.

We believe that the `study` variable is more of a geographical indicator to distinguish different study sites, and it will not be critical in predicting recovery time. Therefore, we will remove  the `study` variable. 

Lastly, we will remove variables `race` and `smoking` since we have created dummy variables for them and we will use the dummy variables in further analyses.

```{r warning=FALSE, message=FALSE}
primary = dat_bind_primary %>% 
  mutate(
  # create dummy variables for categorical variables
    # set up 3 dummy variables for `race`, reference = White:
         race_2 = ifelse(race == 2, 1, 0), 
         race_3 = ifelse(race == 3, 1, 0),
         race_4 = ifelse(race == 4, 1, 0),
    # set up 2 dummy variables for `smoking`, reference = Never smoked:
         smoking_1 = ifelse(smoking == 1, 1, 0), 
         smoking_2 = ifelse(smoking == 2, 1, 0)) 

# remove variables that will not be used
primary = primary %>% 
  select(-study, -race, -smoking) 

# partition again based on the new outcome variable
x <- model.matrix(lrecovery_time ~ ., primary)[trainRows,-1]

y <- primary$lrecovery_time[trainRows]

x2 <- model.matrix(lrecovery_time ~ ., primary)[-trainRows,-1]

y2 <- primary$lrecovery_time[-trainRows]

ctrl1 <- trainControl(method = "cv")
```


# Primary analysis: continuous time to recovery

## Model 1: Linear model
```{r warning=FALSE, message=FALSE}
set.seed(2460)

lm.fit <- train(x, y, 
                method = "glm", 
                preProcess = c("center", "scale"),
                trControl = ctrl1)

summary(lm.fit)

coef(lm.fit$finalModel) %>% round(2) %>% 
  as.matrix() %>% as.data.frame() %>% View()
```

## Model 2: Ridge
```{r warning=FALSE, message=FALSE}
set.seed(2460)

ridge.fit <- train(x, y,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 0,
                                          lambda = exp(seq(-2, -8, length = 100))),
                   preProc = c("center", "scale"),
                   trControl = ctrl1)

ridge.fit$bestTune

plot(ridge.fit, xTrans = log)

coef(ridge.fit$finalModel, s = ridge.fit$bestTune$lambda) %>% 
  round(2) %>% 
  as.matrix() %>% as.data.frame() %>% View()
```


## Model 3: Lasso
```{r warning=FALSE, message=FALSE}
set.seed(2460)

lasso.fit <- train(x, y, 
                   method = "glmnet", 
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(-15, -6, length = 100))),
                   preProcess = c("center", "scale"),
                   trControl = ctrl1)

lasso.fit$bestTune

plot(lasso.fit, xTrans = log)

coef(lasso.fit$finalModel, s = lasso.fit$bestTune$lambda) %>% 
  round(2) %>% 
  as.matrix() %>% as.data.frame() %>% View()
```


## Model 4: Elastic net
```{r warning=FALSE, message=FALSE}
set.seed(2460)

enet.fit <- train(x, y, 
                  method = "glmnet", 
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(-12, -2, 
                                                          length = 100))), 
                  preProcess = c("center", "scale"),
                  trControl = ctrl1)

enet.fit$bestTune

myCol <- rainbow(25)

myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

plot(enet.fit, par.settings = myPar, xTrans = log)

coef(enet.fit$finalModel, s = enet.fit$bestTune$lambda) %>% 
  round(2) %>% 
  as.matrix() %>% as.data.frame() %>% View()
```


## Model 5: Partial least square
```{r warning=FALSE, message=FALSE}
set.seed(2460)

pls.fit <- train(x, y,
                 method = "pls",
                 tuneGrid = data.frame(ncomp = 1:16),
                 trControl = ctrl1,
                 preProcess = c("center", "scale"))

pls.fit$bestTune

ggplot(pls.fit, highlight = TRUE) + theme_bw()

summary(pls.fit)

coef(pls.fit$finalModel) %>% round(2) %>% 
  as.matrix() %>% as.data.frame() %>% View()
```


## Model 6: GAM
```{r warning=FALSE, message=FALSE}
set.seed(2460)

gam.fit <- train(x, y,
                 method = "gam",
                 tuneGrid = data.frame(method = "GCV.Cp", select = c(TRUE,FALSE)),
                 preProcess = c("center", "scale"),
                 trControl = ctrl1)

gam.fit$bestTune

gam.fit$finalModel

summary(gam.fit)
```


### Plot continuous predictors in GAM
```{r warning=FALSE, message=FALSE}
var.names <- c("age", "SBP", "LDL", "bmi", "height", "weight")

# make a matrix for easier comprehension of the plot with 16 predictors
matrix <- matrix(var.names, nrow = 2, ncol = 3, byrow = TRUE)

# use the matrix to correspond each plot with each predictor
print(matrix)

gam.plot <- gam.fit$finalModel

# make 16 plots into one
par(mar = c(3, 3, 2, 2), mfrow = c(2, 3))

plot(gam.plot)

title(main = "Predictors' Plot", cex.main = 1, font.main = 3, outer = TRUE, line = -1)
```


## Model 7: MARS
```{r warning=FALSE, message=FALSE}
mars_grid <- expand.grid(degree = 1:3,
                         nprune = 2:26) 

set.seed(2460)

mars.fit <- train(x, y,
                  method = "earth",
                  tuneGrid = mars_grid,
                  preProcess = c("center", "scale"),
                  trControl = ctrl1)

ggplot(mars.fit, highlight = TRUE)

mars.fit$bestTune

coef(mars.fit$finalModel) %>% round(2) %>% 
  as.matrix() %>% as.data.frame() %>% View()
```

### Partial dependence
```{r warning=FALSE, message=FALSE}
pdp::partial(mars.fit, pred.var = c("age"), grid.resolution = 200) %>% autoplot()
```


## Model 8: Regression tree
```{r warning=FALSE, message=FALSE}
set.seed(2460)

rpart.fit <- train(lrecovery_time ~ . ,
                   primary[trainRows,],
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-9,-4, length = 50))),
                   trControl = ctrl1,
                   preProcess = c("center", "scale"))

rpart.fit$bestTune

ggplot(rpart.fit, highlight = TRUE)

rpart.plot(rpart.fit$finalModel)
```


## Model 9: Random forest 
```{r warning=FALSE, message=FALSE}
rf.grid <- expand.grid(mtry = 1:16, #16 predictors
                       splitrule = "variance",
                       min.node.size = 1:6)

set.seed(2460)

rf.fit <- train(lrecovery_time ~ . ,
                primary[trainRows,],
                method = "ranger",
                tuneGrid = rf.grid,
                trControl = ctrl1,
                preProcess = c("center", "scale"))

rf.fit$bestTune

ggplot(rf.fit, highlight = TRUE)
```

### Variable importance
```{r warning=FALSE, message=FALSE}
vip(rf.fit,
    method = "permute",
    train = primary[trainRows,],
    target = "lrecovery_time",
    metric = "RMSE",
    nsim = 10,
    pred_wrapper = predict,
    geom = "boxplot",
    all_permutations = TRUE,
    mapping = aes_string(fill = "Variable"))
```

### Partial dependence
```{r warning=FALSE, message=FALSE}
pdp1.rf <- rf.fit %>%
  partial(pred.var = c("bmi")) %>%
  autoplot(train = primary[trainRows,], rug = TRUE)

pdp1.rf
```


## Model 10: Boosting
```{r warning=FALSE, message=FALSE}
gbm.grid <- expand.grid(n.trees = c(750,1000,1500,2000,2500,3000,3500),
                        interaction.depth = 1:3,
                        shrinkage = c(0.005,0.01),
                        n.minobsinnode = c(1))

set.seed(2460)

gbm.fit <- train(lrecovery_time ~ . ,
                 primary[trainRows,],
                 method = "gbm",
                 tuneGrid = gbm.grid,
                 trControl = ctrl1,
                 preProcess = c("center", "scale"),
                 verbose = F)

gbm.fit$bestTune

ggplot(gbm.fit, highlight = T)
```


## Model comparison
```{r warning=FALSE, message=FALSE}
res_pri <- resamples(list(lm = lm.fit, 
                          ridge = ridge.fit,
                          lasso = lasso.fit,
                          enet = enet.fit,
                          pls = pls.fit,
                          gam = gam.fit,
                          mars = mars.fit,
                          reg.tree = rpart.fit,
                          rt = rf.fit,
                          boosting = gbm.fit))

summary(res_pri)

bwplot(res_pri, metric = "RMSE")
```



# Secondary analysis: binary time to recovery

## Set up
```{r warning=FALSE, message=FALSE}
# make binary outcome
dat_bind_secondary = dat_bind %>% 
  na.omit(dat_bind) %>% 
  mutate(brecovery_time = ifelse(recovery_time > 30, 1, 0)) %>% 
  mutate(brecovery_time = as.factor(brecovery_time)) %>% 
  mutate(brecovery_time = dplyr::recode(brecovery_time, 
                                 "1" = "Long", "0" = "Short")) %>% 
  dplyr::select(-recovery_time, -id) 
  
# lets check the distribution
binary = dat_bind_secondary %>% 
  ggplot(aes(x = brecovery_time)) + geom_bar() 

binary

# partition again based on the new outcome variable
set.seed(2460)

trainRows_sec <- createDataPartition(y = dat_bind_secondary$brecovery_time, p = 0.8, list = FALSE)

ctrl2 <- trainControl(method = "cv", 
                      summaryFunction = twoClassSummary,
                      classProbs = TRUE)

contrasts(dat_bind_secondary$brecovery_time)
```

## Understand the relationship between continuous predictors and the binary outcome
```{r warning=FALSE, message=FALSE}
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1)
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2)
trellis.par.set(theme1)

# plotting continuous predictors 
featurePlot(x = model.matrix(brecovery_time ~ ., dat_bind_secondary)[trainRows_sec,c("age", "SBP", "LDL", "height", "weight", "bmi")],
            y = dat_bind_secondary$brecovery_time[trainRows_sec],
            plot = "box",
            span = .5,
            labels = c("Recovery Time","Y"),
            layout = c(3,2))
```

## Understand the relationship between categorical predictors and the binary outcome
```{r warning=FALSE, message=FALSE}
gender2 = (dat_bind_secondary[trainRows_sec, -1]) %>% 
  ggplot(aes(x = brecovery_time, fill = as.factor(gender), group = as.factor(gender))) + geom_bar(position = "dodge") + labs(x = "Recovery Time", y = "Count", fill = "Gender") +
  scale_fill_discrete(labels = c("Female", "Male"))


race2 = (dat_bind_secondary[trainRows_sec, -1]) %>% 
  ggplot(aes(x = brecovery_time, fill = as.factor(race), group = as.factor(race))) + geom_bar(position = "dodge") + labs(x = "Recovery Time", y = "Count", fill = "Race") +
  scale_fill_discrete(labels = c("White", "Asian", "Black", "Hispanic"))

smoking2 = (dat_bind_secondary[trainRows_sec, -1]) %>% 
  ggplot(aes(x = brecovery_time, fill = as.factor(smoking), group = as.factor(smoking))) + geom_bar(position = "dodge") + labs(x = "Recovery Time", y = "Count", fill = "Smoking Status") +
  scale_fill_discrete(labels = c("Never smoked", "Former Smoker", "Current smoker"))

hypertension2 = (dat_bind_secondary[trainRows_sec, -1]) %>% 
  ggplot(aes(x = brecovery_time, fill = as.factor(hypertension), group = as.factor(hypertension))) + geom_bar(position = "dodge") + labs(x = "Recovery Time", y = "Count", fill = "Hypertension") +
  scale_fill_discrete(labels = c("No", "Yes"))

diabetes2 = (dat_bind_secondary[trainRows_sec, -1]) %>% 
  ggplot(aes(x = brecovery_time, fill = as.factor(diabetes), group = as.factor(diabetes))) + geom_bar(position = "dodge") + labs(x = "Recovery Time", y = "Count", fill = "Diabetes") +
  scale_fill_discrete(labels = c("No", "Yes"))

vaccine2 = (dat_bind_secondary[trainRows_sec, -1]) %>% 
  ggplot(aes(x = brecovery_time, fill = as.factor(vaccine), group = as.factor(vaccine))) + geom_bar(position = "dodge") + labs(x = "Recovery Time", y = "Count", fill = "Vaccination") +
  scale_fill_discrete(labels = c("No", "Yes"))

severity2 = (dat_bind_secondary[trainRows_sec, -1]) %>% 
  ggplot(aes(x = brecovery_time, fill = as.factor(severity), group = as.factor(severity))) + geom_bar(position = "dodge") + labs(x = "Recovery Time", y = "Count", fill = "Severity") +
  scale_fill_discrete(labels = c("Not severe", "Severe"))

study2 = (dat_bind_secondary[trainRows_sec, -1]) %>% 
  ggplot(aes(x = brecovery_time, fill = as.factor(study), group = as.factor(study))) + geom_bar(position = "dodge") + labs(x = "Recovery Time", y = "Count", fill = "Study")

cat_combined_plot2 = ggarrange(gender2, race2, smoking2, hypertension2, 
                               diabetes2, vaccine2, severity2, study2,
                          ncol = 2, nrow = 4)

cat_combined_plot2
```

## Make dummy variables
```{r warning=FALSE, message=FALSE}
# dummy variable creation

secondary = dat_bind_secondary %>% 
  mutate(
  # create dummy variables for categorical variables
    # set up 3 dummy variables for `race`, reference = White:
         race_2 = ifelse(race == 2, 1, 0), 
         race_3 = ifelse(race == 3, 1, 0),
         race_4 = ifelse(race == 4, 1, 0),
    # set up 2 dummy variables for `smoking`, reference = Never smoked:
         smoking_1 = ifelse(smoking == 1, 1, 0), 
         smoking_2 = ifelse(smoking == 2, 1, 0)) 

# remove variables that will not be used
secondary = secondary %>% 
  dplyr::select(-study, -race, -smoking) %>% 
  dplyr::select(brecovery_time, everything()) #arrange variable orders
```


## Model 1: Logistic regression
```{r warning=FALSE, message=FALSE}
set.seed(2460)

model.glm <- train(x = secondary[trainRows_sec,2:17], 
                   y = secondary$brecovery_time[trainRows_sec],
                   method = "glm",
                   metric = "ROC",
                   trControl = ctrl2,
                   preProcess = c("center", "scale"))

summary(model.glm)
```

## Model 2: MARS
```{r warning=FALSE, message=FALSE}
set.seed(2460)

model.mars <- train(x = secondary[trainRows_sec,2:17], 
                    y = secondary$brecovery_time[trainRows_sec],
                    method = "earth",
                    tuneGrid = expand.grid(degree = 1:3,
                                           nprune = 2:26),
                    metric = "ROC",
                    trControl = ctrl2,
                    preProcess = c("center", "scale"))

plot(model.mars)

model.mars$bestTune

coef(model.mars$finalModel)
```

## Model 3: LDA
```{r warning=FALSE, message=FALSE}
set.seed(2460)

model.lda <- train(x = secondary[trainRows_sec,2:17], 
                   y = secondary$brecovery_time[trainRows_sec],
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl2,
                   preProcess = c("center", "scale"))

summary(model.lda)
```

## Model 4: Classification tree
```{r warning=FALSE, message=FALSE}
set.seed(2460)

model.rpart <- train(brecovery_time ~ . , secondary, 
                     subset = trainRows_sec,
                     method = "rpart",
                     tuneGrid = data.frame(cp = exp(seq(-20,-5, len = 100))),
                     trControl = ctrl2,
                     metric = "ROC",
                     preProcess = c("center", "scale"))

model.rpart$bestTune

ggplot(model.rpart, highlight = TRUE)

rpart.plot(model.rpart$finalModel)
```

## Model 5: Random forest
```{r warning=FALSE, message=FALSE}
rf.grid.sec <- expand.grid(mtry = 1:16,
                           splitrule = "gini",
                           min.node.size = seq(from = 2, to = 10, by = 2))

set.seed(2460)

model.rf <- train(brecovery_time ~ . , secondary, 
                  subset = trainRows_sec,
                  method = "ranger",
                  tuneGrid = rf.grid.sec,
                  metric = "ROC",
                  trControl = ctrl2,
                  preProcess = c("center", "scale"))

ggplot(model.rf, highlight = TRUE)

model.rf$bestTune
```

## Model 6: Boosting
```{r warning=FALSE, message=FALSE}
gbmA.grid <- expand.grid(n.trees = c(1000,2000,3000,4000,5000),
                         interaction.depth = 1:6,
                         shrinkage = c(0.0005,0.001,0.002),
                         n.minobsinnode = 1)

set.seed(2460)

model.gbmA <- train(brecovery_time ~ . , secondary, 
                    subset = trainRows_sec,
                    tuneGrid = gbmA.grid,
                    trControl = ctrl2,
                    method = "gbm",
                    distribution = "adaboost",
                    metric = "ROC",
                    preProcess = c("center", "scale"),
                    verbose = FALSE)

ggplot(model.gbmA, highlight = TRUE)

model.gbmA$bestTune
```

## Model comparison
```{r warning=FALSE, message=FALSE}
res_sec <- resamples(list(logistic = model.glm,
                          mars = model.mars,
                          lda = model.lda,
                          classification.tree = model.rpart,
                          rf = model.rf,
                          boosting = model.gbmA))

summary(res_sec)

bwplot(res_sec, metric = "ROC")
```

