---
title: "Data Science 2 Final"
author: "Group 6"
output: 
  pdf_document:
    toc: TRUE
---

\newpage

# Set up

## Load libraries and data
```{r warning=FALSE, message=FALSE}
library(caret)
library(mgcv)
library(earth)
library(tidyverse)
library(summarytools)
library(corrplot)
library(ggpubr)
library(rpart)
library(rpart.plot)
library(randomForest)
library(ranger)
library(gbm)

setwd("D:/CUMC/Y2S2/DS2/Final/ds2_final")

load("./recovery.RData") 
```

## Subset 2 df and keep unique observations
```{r warning=FALSE, message=FALSE}
set.seed(2543) 

dat1 <- dat[sample(1:10000, 2000),]

set.seed(4017) 

dat2 <- dat[sample(1:10000, 2000),]

dat_bind <- unique(rbind(dat1, dat2))
```

# Exploratory analysis and data visualization

## Data Partition

Here, we mainly want to investigate the EDA of the training dataset. Therefore, we will start with the data partition. 

```{r warning=FALSE, message=FALSE}
set.seed(2460)

trainRows <- createDataPartition(y = dat_bind$recovery_time, 
                                 p = 0.8, list = FALSE)
```


## Understanding the outcome variable `recovery_time`
```{r warning=FALSE, message=FALSE}
# check the outcome variable
hist(dat_bind$recovery_time[trainRows], breaks = 50)
```

The distribution of the outcome variable `recovery_time` is heavily right-skewed. To account for this, I will take the log-transformation of the outcome and use that variable for following analyses.


```{r warning=FALSE, message=FALSE}
dat_bind_primary = dat_bind %>% 
  na.omit(dat_bind) %>% 
  mutate(lrecovery_time = log(recovery_time)) %>% 
  select(-recovery_time, -id) 
  
# log-transformation helped with making it more normal
hist(dat_bind_primary$lrecovery_time[trainRows], breaks = 50)
```


## Summary of the dataset
```{r warning=FALSE, message=FALSE}
st_options(plain.ascii = F,
           style = "rmarkdown",
           dfSummary.silent = T,
           footnote = NA,
           subtitle.emphasis = F)

dfSummary(dat_bind_primary[trainRows, -1])
```


## Understand categorical variables
```{r warning=FALSE, message=FALSE}
gender = (dat_bind_primary[trainRows, -1]) %>% 
  ggplot(aes(x = gender)) +  geom_bar() +  labs(x = "Gender", y = "Count")

race = (dat_bind_primary[trainRows, -1]) %>% 
  ggplot(aes(x = race)) + geom_bar() + labs(x = "Race",y = "Count")

smoking = (dat_bind_primary[trainRows, -1]) %>% 
  ggplot(aes(x = smoking)) + geom_bar() + labs(x = "Smoking", y = "Count")

hypertension = (dat_bind_primary[trainRows, -1]) %>% 
  ggplot(aes(x = hypertension)) + geom_bar() + labs(x = "Hypertension", 
                                                    y = "Count")
diabetes = (dat_bind_primary[trainRows, -1]) %>% 
  ggplot(aes(x = diabetes)) + geom_bar() + labs(x = "Diabetes",y = "Count")

vaccine = (dat_bind_primary[trainRows, -1]) %>% 
  ggplot(aes(x = vaccine)) + geom_bar() + labs(x = "Vaccination status",
                                               y = "Count")
severity = (dat_bind_primary[trainRows, -1]) %>% 
  ggplot(aes(x = severity)) + geom_bar() + labs(x = "Severity", y = "Count")

study = (dat_bind_primary[trainRows, -1]) %>% 
  ggplot(aes(x = study)) + geom_bar() + labs(x = "Study Site", y = "Count")

cat_combined_plot = ggarrange(gender, race, smoking, hypertension, 
                               diabetes, vaccine,severity, study,
                          ncol = 2, nrow = 4)

cat_combined_plot
```


## Understand continuous variables
```{r warning=FALSE, message=FALSE}
par(mar = c(3, 3, 2, 2), mfrow = c(2, 3))

age = hist(dat_bind_primary$age[trainRows], breaks = 50)

bmi = hist(dat_bind_primary$bmi[trainRows], breaks = 50)

height = hist(dat_bind_primary$height[trainRows], breaks = 50)

weight = hist(dat_bind_primary$weight[trainRows], breaks = 50)

SBP = hist(dat_bind_primary$SBP[trainRows], breaks = 50)

LDL = hist(dat_bind_primary$LDL[trainRows], breaks = 50)
```


## Understand the correlation between continuous predictors ****
```{r warning=FALSE, message=FALSE}
correlation <- model.matrix(lrecovery_time ~ ., dat_bind_primary)[trainRows,-1]

corrplot(cor(correlation), method = "circle", type = "full")
```


## Understand the relationship with continuous predictors and the outcome
```{r warning=FALSE, message=FALSE}
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1)
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2)
trellis.par.set(theme1)

# plotting continuous predictors 
featurePlot(x = model.matrix(lrecovery_time ~ ., dat_bind_primary)[trainRows,c("age", "SBP", "LDL", "height", "weight", "bmi")],
            y = dat_bind_primary$lrecovery_time[trainRows],
            plot = "scatter",
            span = .5,
            labels = c("Predictors","Y"),
            type = c("p", "smooth"),
            layout = c(3,2))
```


## Considering variables based on the EDA

From the correlation plot, we can observe that `bmi` is highly correlated with `weight` and `height`, which makes sense because BMI is calculated by weight divided by the square of height. This demonstrates collinearity between the variables, and to account for this, I will remove the `bmi` variable for the predictions. 

Also, I believe that the `study` variable is more of a geographical indicator to distinguish different study sites, and it will not be critical in predicting recovery time. Therefore, I will also remove  the `study` variable. 

Lastly, I will remove variables `race` and `smoking` since I have created dummy variables for them and I will use the dummy variables in further analyses.

```{r warning=FALSE, message=FALSE}
primary = dat_bind_primary %>% 
  mutate(
  # create dummy variables for categorical variables
    # set up 3 dummy variables for `race`, reference = White:
         race_2 = ifelse(race == 2, 1, 0), 
         race_3 = ifelse(race == 3, 1, 0),
         race_4 = ifelse(race == 4, 1, 0),
    # set up 2 dummy variables for `smoking`, reference = Never smoked:
         smoking_1 = ifelse(smoking == 1, 1, 0), 
         smoking_2 = ifelse(smoking == 2, 1, 0)) 

# remove variables that will not be used
primary = primary %>% 
  select(-bmi, -study, -race, -smoking) 

# partition again based on the new outcome variable
set.seed(2460)

trainRows_pri <- createDataPartition(y = primary$lrecovery_time, p = 0.8, list = FALSE)

x <- model.matrix(lrecovery_time ~ ., primary)[trainRows_pri,-1]

y <- primary$lrecovery_time[trainRows_pri]

x2 <- model.matrix(lrecovery_time ~ ., primary)[-trainRows_pri,-1]

y2 <- primary$lrecovery_time[-trainRows_pri]

ctrl1 <- trainControl(method = "cv")
```


# Primary analysis: continuous time to recovery

## Mode 1: linear model

### Train the model
```{r warning=FALSE, message=FALSE}
set.seed(2460)

lm.fit <- train(x, y, 
                method = "glm", 
                preProcess = c("center", "scale"),
                trControl = ctrl1)

summary(lm.fit)
```


## Model 2: Ridge

### Train the model
```{r warning=FALSE, message=FALSE}
set.seed(2460)

ridge.fit <- train(x, y,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 0,
                                          lambda = exp(seq(-1, -7, length=100))),
                   preProc = c("center", "scale"),
                   trControl = ctrl1)

ridge.fit$bestTune

plot(ridge.fit, xTrans = log)

coef(ridge.fit$finalModel, s = ridge.fit$bestTune$lambda)
```


## Model 3: Lasso
```{r warning=FALSE, message=FALSE}
set.seed(2460)

lasso.fit <- train(x, y, 
                   method = "glmnet", 
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(-9, -3, length = 100))),
                   preProcess = c("center", "scale"),
                   trControl = ctrl1)

lasso.fit$bestTune

plot(lasso.fit, xTrans = log)

coef(lasso.fit$finalModel, s = lasso.fit$bestTune$lambda)
```


## Model 4: Elastic net
```{r warning=FALSE, message=FALSE}
set.seed(2460)

enet.fit <- train(x, y, 
                  method = "glmnet", 
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(-8, 1, length = 100))), 
                  preProcess = c("center", "scale"),
                  trControl = ctrl1)

enet.fit$bestTune

myCol <- rainbow(25)

myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

plot(enet.fit, par.settings = myPar, xTrans = log)

coef(enet.fit$finalModel, s = enet.fit$bestTune$lambda)
```


## Model 5: Partial least square
```{r warning=FALSE, message=FALSE}
set.seed(2460)

pls.fit <- train(x, y,
                 method = "pls",
                 tuneGrid = data.frame(ncomp = 1:15),
                 trControl = ctrl1,
                 preProcess = c("center", "scale"))

pls.fit$bestTune

ggplot(pls.fit, highlight = TRUE) + theme_bw()

summary(pls.fit)

coef(pls.fit$finalModel)
```


## Model 6: GAM
```{r warning=FALSE, message=FALSE}
set.seed(2460)

gam.fit <- train(x, y,
                 method = "gam",
                 tuneGrid = data.frame(method = "GCV.Cp", select = c(TRUE,FALSE)),
                 preProcess = c("center", "scale"),
                 trControl = ctrl1)

gam.fit$bestTune

gam.fit$finalModel

summary(gam.fit)
```


### Plot continuous predictors in GAM
```{r warning=FALSE, message=FALSE}
var.names <- c("age", "SBP", "LDL", "height", "weight", "-")

# make a matrix for easier comprehension of the plot with 16 predictors
matrix <- matrix(var.names, nrow = 2, ncol = 3, byrow = TRUE)

# use the matrix to correspond each plot with each predictor
print(matrix)

gam.plot <- gam.fit$finalModel

# make 16 plots into one
par(mar = c(3, 3, 2, 2), mfrow = c(2, 3))

plot(gam.plot)

title(main = "Predictors' Plot", cex.main = 1, font.main = 3, outer = TRUE, line = -1)
```


## Model 7: MARS
```{r warning=FALSE, message=FALSE}
mars_grid <- expand.grid(degree = 1:3,
                         nprune = 2:25) 

set.seed(2460)

mars.fit <- train(x, y,
                  method = "earth",
                  tuneGrid = mars_grid,
                  preProcess = c("center", "scale"),
                  trControl = ctrl1)

ggplot(mars.fit, highlight = TRUE)

mars.fit$bestTune

coef(mars.fit$finalModel)
```


## Model 8: Regression tree
```{r warning=FALSE, message=FALSE}
set.seed(2460)

rpart.fit <- train(lrecovery_time ~ . ,
                   primary[trainRows_pri,],
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-9,-5, length = 50))),
                   trControl = ctrl1)

ggplot(rpart.fit, highlight = TRUE)

rpart.plot(rpart.fit$finalModel)
```


## Model 9: Random forest 
```{r warning=FALSE, message=FALSE}
rf.grid <- expand.grid(mtry = 1:15, #15 predictors
                       splitrule = "variance",
                       min.node.size = 1:6)

set.seed(2460)

rf.fit <- train(lrecovery_time ~ . ,
                primary[trainRows_pri,],
                method = "ranger",
                tuneGrid = rf.grid,
                trControl = ctrl1)

ggplot(rf.fit, highlight = TRUE)
```


## Model 10: Boosting
```{r warning=FALSE, message=FALSE}
gbm.grid <- expand.grid(n.trees = c(750,1000,1500,2000,2500,3000,3500),
                        interaction.depth = 1:3,
                        shrinkage = c(0.005,0.01),
                        n.minobsinnode = c(1))

set.seed(2460)

gbm.fit <- train(lrecovery_time ~ . ,
                primary[trainRows_pri,],
                method = "gbm",
                tuneGrid = gbm.grid,
                trControl = ctrl1,
                verbose = F)

ggplot(gbm.fit, highlight = T)
```


## Model comparison
```{r warning=FALSE, message=FALSE}
res_pri <- resamples(list(lm = lm.fit, 
                      ridge = ridge.fit,
                      lasso = lasso.fit,
                      enet = enet.fit,
                      pls = pls.fit,
                      gam = gam.fit,
                      mars = mars.fit,
                      reg.tree = rpart.fit,
                      rt = rf.fit,
                      boosting = gbm.fit))

summary(res_pri)

bwplot(res_pri, metric = "RMSE")
```



# Secondary analysis: binary time to recovery

## Set up
```{r warning=FALSE, message=FALSE}
# make binary outcome
dat_bind_secondary = dat_bind %>% 
  na.omit(dat_bind) %>% 
  mutate(brecovery_time = ifelse(recovery_time > 30, 1, 0)) %>% 
  mutate(brecovery_time = as.factor(brecovery_time)) %>% 
  mutate(brecovery_time = recode(brecovery_time, 
                                 "1" = "Long", "0" = "Short")) %>% 
  select(-recovery_time, -id) 
  
# lets check the distribution
binary = (dat_bind_secondary[trainRows, -1]) %>% 
  ggplot(aes(x = brecovery_time)) + geom_bar() 

binary

secondary = dat_bind_secondary %>% 
  mutate(
  # create dummy variables for categorical variables
    # set up 3 dummy variables for `race`, reference = White:
         race_2 = ifelse(race == 2, 1, 0), 
         race_3 = ifelse(race == 3, 1, 0),
         race_4 = ifelse(race == 4, 1, 0),
    # set up 2 dummy variables for `smoking`, reference = Never smoked:
         smoking_1 = ifelse(smoking == 1, 1, 0), 
         smoking_2 = ifelse(smoking == 2, 1, 0)) 

# remove variables that will not be used
secondary = secondary %>% 
  select(-bmi, -study, -race, -smoking) %>% 
  select(brecovery_time, everything()) #arrange variable orders

# partition again based on the new outcome variable
set.seed(2460)

trainRows_sec <- createDataPartition(y = secondary$brecovery_time, p = 0.8, list = FALSE)

ctrl2 <- trainControl(method = "cv", 
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

contrasts(secondary$brecovery_time)
```

## Model 1: Logistic regression
```{r warning=FALSE, message=FALSE}
set.seed(2460)

model.glm <- train(x = secondary[trainRows_sec,2:16], 
                   y = secondary$brecovery_time[trainRows_sec],
                   method = "glm",
                   metric = "ROC",
                   trControl = ctrl2)

summary(model.glm)
```

## Model 2: MARS
```{r warning=FALSE, message=FALSE}
set.seed(2460)

model.mars <- train(x = secondary[trainRows_sec,2:16], 
                    y = secondary$brecovery_time[trainRows_sec],
                    method = "earth",
                    tuneGrid = expand.grid(degree = 1:3,
                                           nprune = 2:25),
                    metric = "ROC",
                    trControl = ctrl2)

plot(model.mars)

model.mars$bestTune

coef(model.mars$finalModel)
```

## Model 3: LDA
```{r warning=FALSE, message=FALSE}
set.seed(2460)

model.lda <- train(x = secondary[trainRows_sec,2:16], 
                   y = secondary$brecovery_time[trainRows_sec],
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl2)

summary(model.lda)
```

## Model 4: Classification tree
```{r warning=FALSE, message=FALSE}
set.seed(2460)

model.rpart <- train(brecovery_time ~ . , secondary, 
                     subset = trainRows_sec,
                     method = "rpart",
                     tuneGrid = data.frame(cp = exp(seq(-14,-6, len = 50))),
                     trControl = ctrl2,
                     metric = "ROC")

ggplot(model.rpart, highlight = TRUE)
```

## Model 5: Random forrest
```{r}
rf.grid.sec <- expand.grid(mtry = 1:16,
                       splitrule = "gini",
                       min.node.size = seq(from = 2, to = 10, by = 2))

set.seed(2460)

model.rf <- train(brecovery_time ~ . , secondary, 
                  subset = trainRows_sec,
                  method = "ranger",
                  tuneGrid = rf.grid.sec,
                  metric = "ROC",
                  trControl = ctrl2)

ggplot(model.rf, highlight = TRUE)
```

## Model 6: Boosting
```{r warning=FALSE, message=FALSE}
gbmA.grid <- expand.grid(n.trees = c(1000,2000,3000,4000,5000),
                         interaction.depth = 1:6,
                         shrinkage = c(0.0005,0.001,0.002),
                         n.minobsinnode = 1)

set.seed(2460)

model.gbmA <- train(brecovery_time ~ . , secondary, 
                    subset = trainRows_sec,
                    tuneGrid = gbmA.grid,
                    trControl = ctrl2,
                    method = "gbm",
                    distribution = "adaboost",
                    metric = "ROC",
                    verbose = FALSE)

ggplot(model.gbmA, highlight = TRUE)
```


## Model comparison
```{r warning=FALSE, message=FALSE}
res_sec <- resamples(list(GLM = model.glm,
                      MARS = model.mars,
                      LDA = model.lda,
                      tree = model.rpart,
                      rf = model.rf,
                      boosting = model.gbmA))

summary(res_sec)

bwplot(res_sec, metric = "ROC")
```




